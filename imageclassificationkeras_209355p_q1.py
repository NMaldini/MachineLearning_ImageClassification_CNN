# -*- coding: utf-8 -*-
"""ImageClassificationKeras_209355P.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19hXs0QrG2OuAyfzGaSS39rlSQqjUVaR-
"""

import tensorflow as tf
import os
import numpy as np

from matplotlib import pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

if not os.path.isdir('models'):
    os.mkdir('models')
    
print('TensorFlow version:', tf.__version__)

#the dataset has been divided into training and test
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

#Labels have been one hot encoded for both training and test
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

#Model Build
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.layers import Dropout, Flatten, Input, Dense

def create_model():

    #created a method becuase pattern is repeating 
    def add_conv_block(model, num_filters):
        #first Convolutional layer with 3 kernals
        model.add(Conv2D(num_filters, 3, activation='relu', padding='same'))
        #Batch normalization. Kind of regulaization (to ensure that lot of covariance shift at the output of preceeding layer)
        model.add(BatchNormalization())

        #Pooling layer with pool size 2. This will reduce rows and coulumns to half of the value
        model.add(MaxPooling2D(pool_size=2))

        #droupout layer
        model.add(Dropout(0.5))
        return model

    #Instantiate Keras Sequential model
    model = tf.keras.models.Sequential()

    #specify input shape
    model.add(Input(shape=(28,28,1)))
    
    #build different Convolutional blocks with different filters count
    model = add_conv_block(model, 28)
    model = add_conv_block(model, 56)
    model = add_conv_block(model, 112)
    
    #output of final Convolutional block will be flatten using flatten layer
    model.add(Flatten())

    #finally dense layer (we need probability distribution across the 10 classes that we have)
    model.add(Dense(10, activation='softmax'))
    
    #model compilation, use accuracy as training matrix
    model.compile(
        loss='categorical_crossentropy',
        optimizer= 'adam', metrics=['accuracy']
    )
    return model

model = create_model()
model.summary()

x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)

x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

x_train_noise = np.clip(x_train_noise, 0., 1.)
x_test_noise = np.clip(x_test_noise, 0., 1.)

x_train_noise.shape

y_train.shape

y_test.shape

h = model.fit(
    
    #x_train valu is normalized(255 is used because pixel values range from 0-255)
    x_train/255., y_train,
    validation_data = (x_test/255., y_test),
    epochs = 20, batch_size =112,

    #used EarlyStopping callback, because I wanted to monitor validation accuracy parameter
    callbacks=[
        #patience 3 epochs
        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),
        tf.keras.callbacks.ModelCheckpoint(
            'models/model_{val_accuracy:.3f}.h5',
            save_best_only=True, save_weights_only=False,
            monitor= 'val_accuracy'
        )
    ]
)

#Display plot

accs = h.history['accuracy']
val_accs = h.history['val_accuracy']

plt.plot(range(len(accs)), accs, label='Training')
plt.plot(range(len(accs)), val_accs, label='Validation')
plt.legend()
plt.show()